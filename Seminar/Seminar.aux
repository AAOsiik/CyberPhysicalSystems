\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{OKEEFE1971171}
\citation{Maguire}
\citation{HippocampalReplaysGirard}
\citation{NeuralDynaQ}
\citation{OKEEFE1971171}
\citation{Nakazawa}
\citation{NeuralDynaQ}
\citation{GUPTA2010695}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Experimental setup}{1}{subsection.1.1}}
\citation{Dyna}
\citation{Dyna}
\citation{NeuralDynaQ}
\citation{Dyna}
\citation{Dyna}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Discretized experimental setup. The agent has to make a decision at points T1 and T2, in which the dicision at T2 leads to one of the rewarding sites. \citep  {NeuralDynaQ}\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:setup}{{1}{2}{Discretized experimental setup. The agent has to make a decision at points T1 and T2, in which the dicision at T2 leads to one of the rewarding sites. \citep {NeuralDynaQ}\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Problem Formulation Used in Dyna. The agent's objective is to maximize the total reward it receives over time. \citep  {Dyna}\relax }}{2}{figure.caption.2}}
\newlabel{fig:dyna}{{2}{2}{The Problem Formulation Used in Dyna. The agent's objective is to maximize the total reward it receives over time. \citep {Dyna}\relax }{figure.caption.2}{}}
\citation{Lecture}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reinforcement Learning}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Markov Decision Problem}{3}{subsection.2.1}}
\newlabel{eq:vvalue}{{1}{3}{Markov Decision Problem}{equation.2.1}{}}
\newlabel{eq:value-iteration}{{2}{3}{Markov Decision Problem}{equation.2.2}{}}
\citation{HatemRL}
\citation{HatemRL}
\citation{Lecture}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The structure of reinforcement learning based on an Artificial Neural Network \citep  {HatemRL}\relax }}{4}{figure.caption.3}}
\newlabel{fig:qlearn}{{3}{4}{The structure of reinforcement learning based on an Artificial Neural Network \citep {HatemRL}\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Q-Learning}{4}{subsection.2.2}}
\newlabel{sec:qlearn}{{2.2}{4}{Q-Learning}{subsection.2.2}{}}
\newlabel{eq:qlearn}{{4}{4}{Q-Learning}{equation.2.4}{}}
\newlabel{eq:qupdate}{{5}{4}{Q-Learning}{equation.2.5}{}}
\citation{Moore93}
\citation{Sutton1998}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Prioritized Sweeping}{5}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}GALMO}{5}{section.3}}
\newlabel{sec:galmo}{{3}{5}{GALMO}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of multiple predecessors in the alternation task. At the last position on the third run (right), the location component of the predecessor state (white mouse) is identical but the memory component is different. \citep  {NeuralDynaQ}\relax }}{5}{figure.caption.4}}
\newlabel{fig:predecessors}{{4}{5}{Example of multiple predecessors in the alternation task. At the last position on the third run (right), the location component of the predecessor state (white mouse) is identical but the memory component is different. \citep {NeuralDynaQ}\relax }{figure.caption.4}{}}
\citation{OKEEFE1971171}
\citation{AdaptiveMixture}
\citation{AdaptiveMixture}
\citation{NeuralDynaQ}
\citation{harrison.2019}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A system of expert and gating networks. Each expert as well as gating network is a feedforward network with same amount of inputs and outputs\citep  {AdaptiveMixture}. Derived scheme.\relax }}{6}{figure.caption.5}}
\newlabel{fig:example}{{5}{6}{A system of expert and gating networks. Each expert as well as gating network is a feedforward network with same amount of inputs and outputs\citep {AdaptiveMixture}. Derived scheme.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Project and Results}{6}{section.4}}
\newlabel{sec:project}{{4}{6}{Project and Results}{section.4}{}}
\bibstyle{plainnat}
\bibdata{./main}
\newlabel{fig:task12}{{6a}{7}{Performance during Task 1 in the project. Results for Task 2 are nearly identical.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:task12}{{a}{7}{Performance during Task 1 in the project. Results for Task 2 are nearly identical.\relax }{figure.caption.6}{}}
\newlabel{fig:task3}{{6b}{7}{Performance during Task 3.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:task3}{{b}{7}{Performance during Task 3.\relax }{figure.caption.6}{}}
\newlabel{fig:galmo}{{6c}{7}{Learning without (left) and with (right) replays \citep {NeuralDynaQ}.\relax }{figure.caption.6}{}}
\newlabel{sub@fig:galmo}{{c}{7}{Learning without (left) and with (right) replays \citep {NeuralDynaQ}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Learning dynamics of the own Q-learning implementation in comparison to GALMO performance}.\relax }}{7}{figure.caption.6}}
\newlabel{fig:tasks}{{6}{7}{\textbf {Learning dynamics of the own Q-learning implementation in comparison to GALMO performance}.\relax }{figure.caption.6}{}}
\bibcite{NeuralDynaQ}{{1}{2018}{{Aubin et~al.}}{{Aubin, Khamassi, and Girard}}}
\bibcite{HippocampalReplaysGirard}{{2}{2018}{{Cazé et~al.}}{{Cazé, Khamassi, Aubin, and Girard}}}
\bibcite{GUPTA2010695}{{3}{2010}{{Gupta et~al.}}{{Gupta, van~der Meer, Touretzky, and Redish}}}
\bibcite{harrison.2019}{{4}{2019}{{Harrison}}{{}}}
\bibcite{HatemRL}{{5}{2009}{{Hatem and Abdessemed}}{{}}}
\bibcite{AdaptiveMixture}{{6}{1991}{{Jacobs et~al.}}{{Jacobs, Jordan, Nowlan, and Hinton}}}
\bibcite{Lecture}{{7}{2019}{{Klein and Abbeel}}{{}}}
\bibcite{Maguire}{{8}{2006}{{Maguire et~al.}}{{Maguire, Nannery, and Spiers}}}
\bibcite{Moore93}{{9}{1993}{{Moore and Atkeson}}{{}}}
\bibcite{Nakazawa}{{10}{2004}{{Nakazawa et~al.}}{{Nakazawa, Mchugh, Wilson, and Tonegawa}}}
\bibcite{OKEEFE1971171}{{11}{1971}{{O'Keefe and Dostrovsky}}{{}}}
\bibcite{Dyna}{{12}{1991}{{Sutton}}{{}}}
\bibcite{Sutton1998}{{13}{2018}{{Sutton and Barto}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\newlabel{sec:conclusion}{{5}{8}{Conclusion}{section.5}{}}

\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{OKEEFE1971171}
\citation{Nakazawa}
\citation{NeuralDynaQ}
\citation{GUPTA2010695}
\citation{NeuralDynaQ}
\citation{NeuralDynaQ}
\citation{Dyna}
\citation{Dyna}
\citation{NeuralDynaQ}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{Dyna}
\citation{Dyna}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The maze is discretized into32 positions (squares). The agent can use 4 discrete actions (N,E,S,W). The inputstateφis the concatenation of 32 location components and two reward memory com-ponents. The location part ofφrepresents the activation of 32 place cells co-locatedwith the maze discrete positions, their activityactdepends on the Manhattan dis-tance of the agent to the cell. \citep  {NeuralDynaQ}}}{2}{figure.1}}
\newlabel{fig:setup}{{1}{2}{The maze is discretized into32 positions (squares). The agent can use 4 discrete actions (N,E,S,W). The inputstateφis the concatenation of 32 location components and two reward memory com-ponents. The location part ofφrepresents the activation of 32 place cells co-locatedwith the maze discrete positions, their activityactdepends on the Manhattan dis-tance of the agent to the cell. \citep {NeuralDynaQ}}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Problem Formulation Used in Dyna. The agent's objective is to maximize the total reward it receives over time. \citep  {Dyna}}}{2}{figure.2}}
\newlabel{fig:dyna}{{2}{2}{The Problem Formulation Used in Dyna. The agent's objective is to maximize the total reward it receives over time. \citep {Dyna}}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reinforcement Learning}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Markov Decision Problem}{2}{subsection.2.1}}
\newlabel{eq:vvalue}{{1}{3}{Markov Decision Problem}{equation.2.1}{}}
\newlabel{eq:value-iteration}{{2}{3}{Markov Decision Problem}{equation.2.2}{}}
\bibstyle{plainnat}
\bibdata{./main}
\bibcite{NeuralDynaQ}{{1}{2018}{{Aubin et~al.}}{{Aubin, Khamassi, and Girard}}}
\bibcite{GUPTA2010695}{{2}{2010}{{Gupta et~al.}}{{Gupta, van~der Meer, Touretzky, and Redish}}}
\bibcite{Nakazawa}{{3}{2004}{{Nakazawa et~al.}}{{Nakazawa, Mchugh, Wilson, and Tonegawa}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Basic Q-Learning.}}{4}{figure.3}}
\newlabel{fig:qlearn}{{3}{4}{Basic Q-Learning}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}GALMO}{4}{section.3}}
\newlabel{sec:galmo}{{3}{4}{GALMO}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Project}{4}{section.4}}
\newlabel{sec:project}{{4}{4}{Project}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{4}{section.5}}
\newlabel{sec:results}{{5}{4}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{4}{section.6}}
\bibcite{OKEEFE1971171}{{4}{1971}{{O'Keefe and Dostrovsky}}{{}}}
\bibcite{Dyna}{{5}{1991}{{Sutton}}{{}}}
